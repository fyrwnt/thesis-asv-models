{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset\n",
    "the features are directly taken from [Borzi](https://github.com/UNICT-Fake-Audio/features-archive/tree/main/datasets/ASVSPOOF_2019_LA_V2) github repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T12:27:50.460830Z",
     "iopub.status.busy": "2025-03-10T12:27:50.460413Z",
     "iopub.status.idle": "2025-03-10T12:27:54.372416Z",
     "shell.execute_reply": "2025-03-10T12:27:54.371293Z",
     "shell.execute_reply.started": "2025-03-10T12:27:50.460773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ganti dataset ngikut upload nnti\n",
    "train = pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "dev = pd.read_csv(\"/kaggle/input/borzi-full/dev_set.csv\")\n",
    "eval = pd.read_csv(\"/kaggle/input/borzi-full/eval_set.csv\")\n",
    "\n",
    "# Ganti value label\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "dev['label'] = dev['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "eval['label'] = eval['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "\n",
    "# Drop col gk penting\n",
    "train = train.drop('AUDIO_FILE_NAME', axis=1)\n",
    "dev = dev.drop('AUDIO_FILE_NAME', axis=1)\n",
    "eval = eval.drop('AUDIO_FILE_NAME', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-16T13:45:13.113355Z",
     "iopub.status.busy": "2025-01-16T13:45:13.112968Z",
     "iopub.status.idle": "2025-01-16T13:46:34.406832Z",
     "shell.execute_reply": "2025-01-16T13:46:34.405559Z",
     "shell.execute_reply.started": "2025-01-16T13:45:13.113301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import os\n",
    "\n",
    "# Concatenate the datasets\n",
    "data = pd.concat([train, dev, eval], axis=0)\n",
    "\n",
    "non_feature_columns = ['SYSTEM_ID', 'label', 'spectral_bandwidth']\n",
    "feature_columns = [col for col in data.columns if col not in non_feature_columns]\n",
    "\n",
    "# Calculate feature variance\n",
    "feature_variances = data[feature_columns].var()\n",
    "\n",
    "# Identify low-variance features (threshold can be adjusted; using 0.01 here as an example)\n",
    "low_variance_threshold = 0.01\n",
    "low_variance_features = feature_variances[feature_variances < low_variance_threshold]\n",
    "\n",
    "# Sort SYSTEM_ID before analysis\n",
    "data['SYSTEM_ID'] = pd.Categorical(data['SYSTEM_ID'], categories=sorted(data['SYSTEM_ID'].unique()), ordered=True)\n",
    "\n",
    "# Plot feature distributions for all features\n",
    "feature_distributions_dir = \"/mnt/data/feature_distributions/\"\n",
    "os.makedirs(feature_distributions_dir, exist_ok=True)\n",
    "\n",
    "# Get a colormap with as many unique colors as there are SYSTEM_IDs\n",
    "unique_system_ids = sorted(data['SYSTEM_ID'].unique())\n",
    "color_map = cm.get_cmap('tab20', len(unique_system_ids))\n",
    "\n",
    "# Save distribution plots for each feature\n",
    "for feature in feature_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for idx, system_id in enumerate(unique_system_ids):\n",
    "        subset = data[data['SYSTEM_ID'] == system_id]\n",
    "        plt.hist(subset[feature], bins=30, alpha=0.5, color=color_map(idx), label=f\"SYSTEM_ID: {system_id}\")\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Return summary of low-variance features\n",
    "low_variance_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-14T13:32:41.556440Z",
     "iopub.status.busy": "2025-01-14T13:32:41.556095Z",
     "iopub.status.idle": "2025-01-14T13:32:41.561767Z",
     "shell.execute_reply": "2025-01-14T13:32:41.560473Z",
     "shell.execute_reply.started": "2025-01-14T13:32:41.556413Z"
    }
   },
   "source": [
    "## 1.2 EER calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:39:02.248459Z",
     "iopub.status.busy": "2025-03-10T12:39:02.248074Z",
     "iopub.status.idle": "2025-03-10T12:39:02.877432Z",
     "shell.execute_reply": "2025-03-10T12:39:02.876272Z",
     "shell.execute_reply.started": "2025-03-10T12:39:02.248425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, accuracy_score\n",
    "def eval_metr(y_true, y_pred, C0, C1, P_target=0.5):\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    fnr = 1 - tpr  # False Negative Rate (Miss Rate)\n",
    "\n",
    "    # Compute EER\n",
    "    abs_diff = np.abs(fpr - fnr)\n",
    "    eer_index = np.argmin(abs_diff)\n",
    "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "\n",
    "    # Compute t-DCF\n",
    "    pi_spoof = P_target  # Prior for spoof\n",
    "    pi_bonafide = 1 - P_target  # Prior for bonafide\n",
    "\n",
    "    # Calculate t-DCF values\n",
    "    tdcf_values = (pi_bonafide * C0 * fnr + pi_spoof * C1 * fpr) / min(pi_bonafide * C0, pi_spoof * C1)\n",
    "\n",
    "    # Find minimum t-DCF\n",
    "    min_tdcf = np.min(tdcf_values)\n",
    "\n",
    "    return eer, min_tdcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Singular feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:13:53.114958Z",
     "iopub.status.busy": "2025-01-08T07:13:53.114359Z",
     "iopub.status.idle": "2025-01-08T07:13:53.967298Z",
     "shell.execute_reply": "2025-01-08T07:13:53.966015Z",
     "shell.execute_reply.started": "2025-01-08T07:13:53.114922Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def random_forest_pipeline(train, dev, eval):\n",
    "    excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "    report_list = []\n",
    "    csv_data = []\n",
    "\n",
    "    # Iterate over all columns except excluded ones\n",
    "    for col_name in train.columns:\n",
    "        if col_name in excluded_columns:\n",
    "            continue\n",
    "\n",
    "        # Check for NaN values and print a warning if present\n",
    "        if train[col_name].isna().any() or dev[col_name].isna().any() or eval[col_name].isna().any():\n",
    "            print(f\"Warning: Column '{col_name}' contains NaN values. Filling with mean.\")\n",
    "\n",
    "        # Step 1: Extract X_set and y_set from train, dev, and eval datasets\n",
    "        X_train, y_train = train[[col_name]].fillna(train[col_name].mean()), train['label']\n",
    "        X_dev, y_dev = dev[[col_name]].fillna(dev[col_name].mean()), dev['label']\n",
    "        X_eval, y_eval = eval[[col_name]].fillna(eval[col_name].mean()), eval['label']\n",
    "\n",
    "        # Step 2: Apply SMOTE to balance the training data\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Step 3: Create and train the Random Forest model\n",
    "        rf_model = RandomForestClassifier(random_state=42)\n",
    "        rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "        # Step 4: Evaluate on dev set\n",
    "        y_dev_pred = rf_model.predict(X_dev)\n",
    "        y_dev_prob = rf_model.predict_proba(X_dev)[:, 1]\n",
    "        accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "        eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "        dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "        # Step 5: Evaluate on eval set\n",
    "        y_eval_pred = rf_model.predict(X_eval)\n",
    "        y_eval_prob = rf_model.predict_proba(X_eval)[:, 1]\n",
    "        accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "        eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "        eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "        # Step 6: Store the results in the text report\n",
    "        report_list.append(f\"=== Evaluation for Feature: {col_name} ===\\n\")\n",
    "        report_list.append(\"\\n=== Evaluation on Dev Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(dev_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\\n\")\n",
    "\n",
    "        report_list.append(\"\\n=== Evaluation on Eval Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(eval_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the results in the CSV data\n",
    "        csv_data.append({\n",
    "            \"Feature\": col_name,\n",
    "            \"Dev Accuracy\": accuracy_dev,\n",
    "            \"Dev EER\": eer_dev * 100,\n",
    "            \"Dev Min t-DCF\": tdcf_dev,\n",
    "            \"Eval Accuracy\": accuracy_eval,\n",
    "            \"Eval EER\": eer_eval * 100,\n",
    "            \"Eval Min t-DCF\": tdcf_eval\n",
    "        })\n",
    "\n",
    "    # Step 8: Save the detailed report to a text file\n",
    "    with open('rf_evaluation_report.txt', 'w') as f:\n",
    "        f.writelines(\"\\n\".join(report_list))\n",
    "\n",
    "    # Step 9: Save the CSV data to a file\n",
    "    df_csv = pd.DataFrame(csv_data)\n",
    "    df_csv.to_csv('rf_evaluation_metrics.csv', index=False)\n",
    "\n",
    "    print(\"Detailed evaluation report saved to rf_evaluation_report.txt\")\n",
    "    print(\"Summary metrics saved to rf_evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:13:54.980325Z",
     "iopub.status.busy": "2025-01-08T07:13:54.979806Z",
     "iopub.status.idle": "2025-01-08T07:18:03.052198Z",
     "shell.execute_reply": "2025-01-08T07:18:03.050840Z",
     "shell.execute_reply.started": "2025-01-08T07:13:54.980294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random_forest_pipeline(train, dev, eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 All Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T07:53:38.123533Z",
     "iopub.status.busy": "2025-01-26T07:53:38.123157Z",
     "iopub.status.idle": "2025-01-26T07:53:38.739615Z",
     "shell.execute_reply": "2025-01-26T07:53:38.738536Z",
     "shell.execute_reply.started": "2025-01-26T07:53:38.123506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def random_forest_pipeline_all(train, dev, eval, excluded_columns):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col not in excluded_columns]\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = rf_model.predict(X_dev)\n",
    "    y_dev_prob = rf_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = rf_model.predict(X_eval)\n",
    "    y_eval_prob = rf_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "    print(\"Random Forest model saved as random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:18:03.071263Z",
     "iopub.status.busy": "2025-01-08T07:18:03.070714Z",
     "iopub.status.idle": "2025-01-08T07:18:23.822957Z",
     "shell.execute_reply": "2025-01-08T07:18:23.821816Z",
     "shell.execute_reply.started": "2025-01-08T07:18:03.071223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "# Assume `data` is a DataFrame that includes features and a 'label' column\n",
    "random_forest_pipeline_all(train, dev, eval, excluded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Singular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:18:23.825095Z",
     "iopub.status.busy": "2025-01-08T07:18:23.824773Z",
     "iopub.status.idle": "2025-01-08T07:18:23.838807Z",
     "shell.execute_reply": "2025-01-08T07:18:23.837247Z",
     "shell.execute_reply.started": "2025-01-08T07:18:23.825068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def adaboost_pipeline(train, dev, eval):\n",
    "    excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "    report_list = []\n",
    "    csv_data = []\n",
    "\n",
    "    # Iterate over all columns except excluded ones\n",
    "    for col_name in train.columns:\n",
    "        if col_name in excluded_columns:\n",
    "            continue\n",
    "\n",
    "        # Check for NaN values and print a warning if present\n",
    "        if train[col_name].isna().any() or dev[col_name].isna().any() or eval[col_name].isna().any():\n",
    "            print(f\"Warning: Column '{col_name}' contains NaN values. Filling with mean.\")\n",
    "\n",
    "        # Step 1: Extract X_set and y_set from train, dev, and eval datasets\n",
    "        X_train, y_train = train[[col_name]].fillna(train[col_name].mean()), train['label']\n",
    "        X_dev, y_dev = dev[[col_name]].fillna(dev[col_name].mean()), dev['label']\n",
    "        X_eval, y_eval = eval[[col_name]].fillna(eval[col_name].mean()), eval['label']\n",
    "\n",
    "        # Step 2: Apply SMOTE to balance the training data\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Step 3: Create and train the Random Forest model\n",
    "        ada_model = AdaBoostClassifier(random_state=42)\n",
    "        ada_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "        # Step 4: Evaluate on dev set\n",
    "        y_dev_pred = ada_model.predict(X_dev)\n",
    "        y_dev_prob = ada_model.predict_proba(X_dev)[:, 1]\n",
    "        accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "        eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "        dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "        # Step 5: Evaluate on eval set\n",
    "        y_eval_pred = ada_model.predict(X_eval)\n",
    "        y_eval_prob = ada_model.predict_proba(X_eval)[:, 1]\n",
    "        accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "        eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "        eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "        # Step 6: Store the results in the text report\n",
    "        report_list.append(f\"=== Evaluation for Feature: {col_name} ===\\n\")\n",
    "        report_list.append(\"\\n=== Evaluation on Dev Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(dev_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\\n\")\n",
    "\n",
    "        report_list.append(\"\\n=== Evaluation on Eval Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(eval_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the results in the CSV data\n",
    "        csv_data.append({\n",
    "            \"Feature\": col_name,\n",
    "            \"Dev Accuracy\": accuracy_dev,\n",
    "            \"Dev EER\": eer_dev * 100,\n",
    "            \"Dev Min t-DCF\": tdcf_dev,\n",
    "            \"Eval Accuracy\": accuracy_eval,\n",
    "            \"Eval EER\": eer_eval * 100,\n",
    "            \"Eval Min t-DCF\": tdcf_eval\n",
    "        })\n",
    "\n",
    "    # Step 8: Save the detailed report to a text file\n",
    "    with open('ada_evaluation_report.txt', 'w') as f:\n",
    "        f.writelines(\"\\n\".join(report_list))\n",
    "\n",
    "    # Step 9: Save the CSV data to a file\n",
    "    df_csv = pd.DataFrame(csv_data)\n",
    "    df_csv.to_csv('ada_evaluation_metrics.csv', index=False)\n",
    "\n",
    "    print(\"Detailed evaluation report saved to ada_evaluation_report.txt\")\n",
    "    print(\"Summary metrics saved to ada_evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:18:23.840296Z",
     "iopub.status.busy": "2025-01-08T07:18:23.839973Z",
     "iopub.status.idle": "2025-01-08T07:19:49.499560Z",
     "shell.execute_reply": "2025-01-08T07:19:49.498344Z",
     "shell.execute_reply.started": "2025-01-08T07:18:23.840269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "adaboost_pipeline(train, dev, eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T07:53:45.475183Z",
     "iopub.status.busy": "2025-01-26T07:53:45.474648Z",
     "iopub.status.idle": "2025-01-26T07:53:45.485180Z",
     "shell.execute_reply": "2025-01-26T07:53:45.484114Z",
     "shell.execute_reply.started": "2025-01-26T07:53:45.475151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def adaboost_pipeline_all(train, dev, eval, excluded_columns):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col not in excluded_columns]\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 1: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 2: Create and train the Random Forest model\n",
    "    ada_model = AdaBoostClassifier(random_state=42)\n",
    "    ada_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 3: Evaluate on dev set\n",
    "    y_dev_pred = ada_model.predict(X_dev)\n",
    "    y_dev_prob = ada_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "\n",
    "    # Step 4: Evaluate on eval set\n",
    "    y_eval_pred = ada_model.predict(X_eval)\n",
    "    y_eval_prob = ada_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 5: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 6: Save the trained model\n",
    "    joblib.dump(ada_model, 'adaboost_model.pkl')\n",
    "    print(\"adaboost model saved as adaboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:19:49.515674Z",
     "iopub.status.busy": "2025-01-08T07:19:49.515249Z",
     "iopub.status.idle": "2025-01-08T07:20:02.399548Z",
     "shell.execute_reply": "2025-01-08T07:20:02.397843Z",
     "shell.execute_reply.started": "2025-01-08T07:19:49.515639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "adaboost_pipeline_all(train, dev, eval, excluded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Singular Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:20:55.545861Z",
     "iopub.status.busy": "2025-01-08T07:20:55.545425Z",
     "iopub.status.idle": "2025-01-08T07:20:55.558906Z",
     "shell.execute_reply": "2025-01-08T07:20:55.557494Z",
     "shell.execute_reply.started": "2025-01-08T07:20:55.545825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def xgboost_pipeline(train, dev, eval):\n",
    "    excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "    report_list = []\n",
    "    csv_data = []\n",
    "\n",
    "    # Iterate over all columns except excluded ones\n",
    "    for col_name in train.columns:\n",
    "        if col_name in excluded_columns:\n",
    "            continue\n",
    "\n",
    "        # Check for NaN values and print a warning if present\n",
    "        if train[col_name].isna().any() or dev[col_name].isna().any() or eval[col_name].isna().any():\n",
    "            print(f\"Warning: Column '{col_name}' contains NaN values. Filling with mean.\")\n",
    "\n",
    "        # Step 1: Extract X_set and y_set from train, dev, and eval datasets\n",
    "        X_train, y_train = train[[col_name]].fillna(train[col_name].mean()), train['label']\n",
    "        X_dev, y_dev = dev[[col_name]].fillna(dev[col_name].mean()), dev['label']\n",
    "        X_eval, y_eval = eval[[col_name]].fillna(eval[col_name].mean()), eval['label']\n",
    "\n",
    "        # Step 2: Apply SMOTE to balance the training data\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Step 3: Create and train the Random Forest model\n",
    "        xgb_model = AdaBoostClassifier(random_state=42)\n",
    "        xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "        # Step 4: Evaluate on dev set\n",
    "        y_dev_pred = xgb_model.predict(X_dev)\n",
    "        y_dev_prob = xgb_model.predict_proba(X_dev)[:, 1]\n",
    "        accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "        eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "        dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "        # Step 5: Evaluate on eval set\n",
    "        y_eval_pred = xgb_model.predict(X_eval)\n",
    "        y_eval_prob = xgb_model.predict_proba(X_eval)[:, 1]\n",
    "        accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "        eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "        eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "        # Step 6: Store the results in the text report\n",
    "        report_list.append(f\"=== Evaluation for Feature: {col_name} ===\\n\")\n",
    "        report_list.append(\"\\n=== Evaluation on Dev Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(dev_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\\n\")\n",
    "\n",
    "        report_list.append(\"\\n=== Evaluation on Eval Set ===\")\n",
    "        report_list.append(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "        report_list.append(\"Classification Report:\")\n",
    "        report_list.append(eval_report)\n",
    "        report_list.append(\"Custom Eval Metric:\")\n",
    "        report_list.append(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "        report_list.append(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\\n\\n\")\n",
    "\n",
    "        # Step 7: Store the results in the CSV data\n",
    "        csv_data.append({\n",
    "            \"Feature\": col_name,\n",
    "            \"Dev Accuracy\": accuracy_dev,\n",
    "            \"Dev EER\": eer_dev * 100,\n",
    "            \"Dev Min t-DCF\": tdcf_dev,\n",
    "            \"Eval Accuracy\": accuracy_eval,\n",
    "            \"Eval EER\": eer_eval * 100,\n",
    "            \"Eval Min t-DCF\": tdcf_eval\n",
    "        })\n",
    "\n",
    "    # Step 8: Save the detailed report to a text file\n",
    "    with open('xgb_evaluation_report.txt', 'w') as f:\n",
    "        f.writelines(\"\\n\".join(report_list))\n",
    "\n",
    "    # Step 9: Save the CSV data to a file\n",
    "    df_csv = pd.DataFrame(csv_data)\n",
    "    df_csv.to_csv('xgb_evaluation_metrics.csv', index=False)\n",
    "\n",
    "    print(\"Detailed evaluation report saved to xgb_evaluation_report.txt\")\n",
    "    print(\"Summary metrics saved to xgb_evaluation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:20:58.280870Z",
     "iopub.status.busy": "2025-01-08T07:20:58.280488Z",
     "iopub.status.idle": "2025-01-08T07:22:24.095065Z",
     "shell.execute_reply": "2025-01-08T07:22:24.093581Z",
     "shell.execute_reply.started": "2025-01-08T07:20:58.280839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgboost_pipeline(train, dev, eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T07:53:48.638616Z",
     "iopub.status.busy": "2025-01-26T07:53:48.638226Z",
     "iopub.status.idle": "2025-01-26T07:53:48.874553Z",
     "shell.execute_reply": "2025-01-26T07:53:48.873680Z",
     "shell.execute_reply.started": "2025-01-26T07:53:48.638557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def xgboost_pipeline_all(train, dev, eval, excluded_columns):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col not in excluded_columns]\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 1: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 2: Create and train the Random Forest model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 3: Evaluate on dev set\n",
    "    y_dev_pred = xgb_model.predict(X_dev)\n",
    "    y_dev_prob = xgb_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "\n",
    "    # Step 4: Evaluate on eval set\n",
    "    y_eval_pred = xgb_model.predict(X_eval)\n",
    "    y_eval_prob = xgb_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "\n",
    "    # Step 5: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 6: Save the trained model\n",
    "    joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "    print(\"xgboost model saved as xgboost_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:22:24.112405Z",
     "iopub.status.busy": "2025-01-08T07:22:24.111984Z",
     "iopub.status.idle": "2025-01-08T07:22:25.807610Z",
     "shell.execute_reply": "2025-01-08T07:22:25.806676Z",
     "shell.execute_reply.started": "2025-01-08T07:22:24.112370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "xgboost_pipeline_all(train, dev, eval, excluded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preselect Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:30.529479Z",
     "iopub.status.busy": "2025-01-08T08:13:30.529154Z",
     "iopub.status.idle": "2025-01-08T08:13:30.541907Z",
     "shell.execute_reply": "2025-01-08T08:13:30.540664Z",
     "shell.execute_reply.started": "2025-01-08T08:13:30.529454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_features(train, dev, eval):\n",
    "    SELECT_FEATURES = ['bit_rate', 'lfcc', 'mindom', 'size', 'spectral_flatness', 'spectral_centroid', 'spectral_mean', 'spectral_rms', 'spectral_spread']\n",
    "\n",
    "    # Step 1: Select only the columns in SELECT_FEATURES plus 'label'\n",
    "    selected_columns = SELECT_FEATURES + ['label']\n",
    "\n",
    "    # Filter the datasets to include only the selected columns\n",
    "    train_filtered = train[selected_columns]\n",
    "    dev_filtered = dev[selected_columns]\n",
    "    eval_filtered = eval[selected_columns]\n",
    "\n",
    "    return train_filtered, dev_filtered, eval_filtered\n",
    "\n",
    "train_filtered, dev_filtered, eval_filtered = filter_features(train, dev, eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:34.106919Z",
     "iopub.status.busy": "2025-01-08T08:13:34.106491Z",
     "iopub.status.idle": "2025-01-08T08:13:34.118152Z",
     "shell.execute_reply": "2025-01-08T08:13:34.116801Z",
     "shell.execute_reply.started": "2025-01-08T08:13:34.106889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def rf_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = rf_model.predict(X_dev)\n",
    "    y_dev_prob = rf_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = rf_model.predict(X_eval)\n",
    "    y_eval_prob = rf_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(rf_model, 'rf_select_model.pkl')\n",
    "    print(\"Random Forest model saved as rf_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:34.631110Z",
     "iopub.status.busy": "2025-01-08T08:13:34.630780Z",
     "iopub.status.idle": "2025-01-08T08:13:50.798739Z",
     "shell.execute_reply": "2025-01-08T08:13:50.797608Z",
     "shell.execute_reply.started": "2025-01-08T08:13:34.631084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:50.800615Z",
     "iopub.status.busy": "2025-01-08T08:13:50.800277Z",
     "iopub.status.idle": "2025-01-08T08:13:50.811610Z",
     "shell.execute_reply": "2025-01-08T08:13:50.810562Z",
     "shell.execute_reply.started": "2025-01-08T08:13:50.800584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def ada_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    ada_model = AdaBoostClassifier(random_state=42)\n",
    "    ada_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = ada_model.predict(X_dev)\n",
    "    y_dev_prob = ada_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = ada_model.predict(X_eval)\n",
    "    y_eval_prob = ada_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(ada_model, 'ada_select_model.pkl')\n",
    "    print(\"Adaboost model saved as ada_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:50.813791Z",
     "iopub.status.busy": "2025-01-08T08:13:50.813445Z",
     "iopub.status.idle": "2025-01-08T08:13:55.227024Z",
     "shell.execute_reply": "2025-01-08T08:13:55.226006Z",
     "shell.execute_reply.started": "2025-01-08T08:13:50.813763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ada_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:55.228300Z",
     "iopub.status.busy": "2025-01-08T08:13:55.228025Z",
     "iopub.status.idle": "2025-01-08T08:13:55.239249Z",
     "shell.execute_reply": "2025-01-08T08:13:55.237775Z",
     "shell.execute_reply.started": "2025-01-08T08:13:55.228276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def xgb_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = xgb_model.predict(X_dev)\n",
    "    y_dev_prob = xgb_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = xgb_model.predict(X_eval)\n",
    "    y_eval_prob = xgb_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(xgb_model, 'xgb_select_model.pkl')\n",
    "    print(\"XGBoost model saved as xgb_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T08:13:55.240689Z",
     "iopub.status.busy": "2025-01-08T08:13:55.240299Z",
     "iopub.status.idle": "2025-01-08T08:13:56.052957Z",
     "shell.execute_reply": "2025-01-08T08:13:56.051412Z",
     "shell.execute_reply.started": "2025-01-08T08:13:55.240654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bayesian Optimization on Select Features\n",
    "Bayesian Optimization using optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Optuna Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:25:02.140348Z",
     "iopub.status.busy": "2025-01-08T09:25:02.139478Z",
     "iopub.status.idle": "2025-01-08T09:25:02.158236Z",
     "shell.execute_reply": "2025-01-08T09:25:02.156046Z",
     "shell.execute_reply.started": "2025-01-08T09:25:02.140307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "SELECT_FEATURES = ['bit_rate', 'lfcc', 'mindom', 'size', 'spectral_flatness', 'spectral_centroid', 'spectral_mean', 'spectral_rms', 'spectral_spread']\n",
    "\n",
    "def prepare_data(train, dev):\n",
    "    # Filter datasets using SELECT_FEATURES\n",
    "    X_train, y_train = train[SELECT_FEATURES], train['label']\n",
    "    X_dev, y_dev = dev[SELECT_FEATURES], dev['label']\n",
    "\n",
    "    # Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    return X_train_smote, y_train_smote, X_dev, y_dev\n",
    "    \n",
    "def objective(trial, model_type, X_train, y_train, X_dev, y_dev):\n",
    "    if model_type == 'RandomForest':\n",
    "        # Hyperparameters for Random Forest\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        max_depth = trial.suggest_int('max_depth', 10, 100)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == 'AdaBoost':\n",
    "        # Hyperparameters for AdaBoost\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 1.0)\n",
    "        model = AdaBoostClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    elif model_type == 'XGBoost':\n",
    "        # Hyperparameters for XGBoost\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "        subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            subsample=subsample,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_dev_prob = model.predict_proba(X_dev)[:, 1]\n",
    "    eer, min_tdcf = eval_metr(y_dev, y_dev_prob, C0=0.1588, C1=2.1007)\n",
    "\n",
    "    return eer\n",
    "\n",
    "# Function to optimize models using Optuna\n",
    "def optimize_model(train, dev, model_type):\n",
    "    X_train, y_train, X_dev, y_dev = prepare_data(train, dev)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')  # Minimize EER\n",
    "    study.optimize(lambda trial: objective(trial, model_type, X_train, y_train, X_dev, y_dev), n_trials=50)\n",
    "\n",
    "    print(f\"Best trial for {model_type}: {study.best_trial.params}\")\n",
    "    print(f\"Best EER for {model_type}: {study.best_value:.4f}\")\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-08T09:25:05.112480Z",
     "iopub.status.busy": "2025-01-08T09:25:05.112098Z",
     "iopub.status.idle": "2025-01-08T09:53:33.386151Z",
     "shell.execute_reply": "2025-01-08T09:53:33.384428Z",
     "shell.execute_reply.started": "2025-01-08T09:25:05.112447Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimize Random Forest\n",
    "rf_study = optimize_model(train, eval, 'RandomForest')\n",
    "\n",
    "# Optimize AdaBoost\n",
    "ada_study = optimize_model(train, eval, 'AdaBoost')\n",
    "\n",
    "# Optimize XGBoost\n",
    "xgb_study = optimize_model(train, eval, 'XGBoost')\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(rf_study.best_trial.params, 'best_rf_model_params.pkl')\n",
    "joblib.dump(ada_study.best_trial.params, 'best_adaboost_model_params.pkl')\n",
    "joblib.dump(xgb_study.best_trial.params, 'best_xgboost_model_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:53:47.535945Z",
     "iopub.status.busy": "2025-01-08T09:53:47.535387Z",
     "iopub.status.idle": "2025-01-08T09:53:47.547296Z",
     "shell.execute_reply": "2025-01-08T09:53:47.546011Z",
     "shell.execute_reply.started": "2025-01-08T09:53:47.535904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_features(train, dev, eval):\n",
    "    SELECT_FEATURES = ['bit_rate', 'lfcc', 'mindom', 'spectral_flatness', 'spectral_centroid', 'spectral_mean', 'spectral_rms', 'spectral_spread']\n",
    "\n",
    "    # Step 1: Select only the columns in SELECT_FEATURES plus 'label'\n",
    "    selected_columns = SELECT_FEATURES + ['label']\n",
    "\n",
    "    # Filter the datasets to include only the selected columns\n",
    "    train_filtered = train[selected_columns]\n",
    "    dev_filtered = dev[selected_columns]\n",
    "    eval_filtered = eval[selected_columns]\n",
    "\n",
    "    return train_filtered, dev_filtered, eval_filtered\n",
    "\n",
    "train_filtered, dev_filtered, eval_filtered = filter_features(train, dev, eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:56:01.150843Z",
     "iopub.status.busy": "2025-01-08T09:56:01.150452Z",
     "iopub.status.idle": "2025-01-08T09:56:01.162540Z",
     "shell.execute_reply": "2025-01-08T09:56:01.161194Z",
     "shell.execute_reply.started": "2025-01-08T09:56:01.150812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 191, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 5}\n",
    "def rf_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42, **params)\n",
    "    rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = rf_model.predict(X_dev)\n",
    "    y_dev_prob = rf_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = rf_model.predict(X_eval)\n",
    "    y_eval_prob = rf_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(rf_model, 'rf_select_model.pkl')\n",
    "    print(\"Random Forest model saved as rf_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:56:01.165205Z",
     "iopub.status.busy": "2025-01-08T09:56:01.164740Z",
     "iopub.status.idle": "2025-01-08T09:56:17.801373Z",
     "shell.execute_reply": "2025-01-08T09:56:17.799772Z",
     "shell.execute_reply.started": "2025-01-08T09:56:01.165171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:55:50.967913Z",
     "iopub.status.busy": "2025-01-08T09:55:50.967477Z",
     "iopub.status.idle": "2025-01-08T09:55:50.978802Z",
     "shell.execute_reply": "2025-01-08T09:55:50.977567Z",
     "shell.execute_reply.started": "2025-01-08T09:55:50.967880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 116, 'learning_rate': 0.05192761614314263}\n",
    "def ada_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    ada_model = AdaBoostClassifier(random_state=42, **params)\n",
    "    ada_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = ada_model.predict(X_dev)\n",
    "    y_dev_prob = ada_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = ada_model.predict(X_eval)\n",
    "    y_eval_prob = ada_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(ada_model, 'ada_select_model.pkl')\n",
    "    print(\"Adaboost model saved as ada_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:55:51.189880Z",
     "iopub.status.busy": "2025-01-08T09:55:51.189418Z",
     "iopub.status.idle": "2025-01-08T09:56:01.149044Z",
     "shell.execute_reply": "2025-01-08T09:56:01.147855Z",
     "shell.execute_reply.started": "2025-01-08T09:55:51.189818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ada_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:54:21.602723Z",
     "iopub.status.busy": "2025-01-08T09:54:21.602341Z",
     "iopub.status.idle": "2025-01-08T09:54:21.614836Z",
     "shell.execute_reply": "2025-01-08T09:54:21.613554Z",
     "shell.execute_reply.started": "2025-01-08T09:54:21.602692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 68, 'max_depth': 7, 'learning_rate': 0.026718036292448403, 'subsample': 0.6261578971619668}\n",
    "def xgb_pipeline_select(train, dev, eval):\n",
    "    # Step 1: Prepare the data\n",
    "    feature_columns = [col for col in train.columns if col != 'label']\n",
    "    \n",
    "    # Function to impute NaN values with column mean\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Impute missing values in train, dev, and eval datasets\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    # Step 2: Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 3: Create and train the Random Forest model\n",
    "    xgb_model = XGBClassifier(random_state=42, **params)\n",
    "    xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    # Step 4: Evaluate on dev set\n",
    "    y_dev_pred = xgb_model.predict(X_dev)\n",
    "    y_dev_prob = xgb_model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, 0.1588, 2.1007)\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "    \n",
    "    # Step 5: Evaluate on eval set\n",
    "    y_eval_pred = xgb_model.predict(X_eval)\n",
    "    y_eval_prob = xgb_model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, 0.1847, 2.0173)\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    # Step 6: Print out the results\n",
    "    print(\"\\n=== Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Step 7: Save the trained model\n",
    "    joblib.dump(xgb_model, 'xgb_select_model.pkl')\n",
    "    print(\"XGBoost model saved as xgb_select_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T09:54:21.965430Z",
     "iopub.status.busy": "2025-01-08T09:54:21.965061Z",
     "iopub.status.idle": "2025-01-08T09:54:22.676667Z",
     "shell.execute_reply": "2025-01-08T09:54:22.674786Z",
     "shell.execute_reply.started": "2025-01-08T09:54:21.965397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_pipeline_select(train_filtered, dev_filtered, eval_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T04:52:13.618955Z",
     "iopub.status.busy": "2025-01-12T04:52:13.618591Z",
     "iopub.status.idle": "2025-01-12T04:52:13.631107Z",
     "shell.execute_reply": "2025-01-12T04:52:13.630282Z",
     "shell.execute_reply.started": "2025-01-12T04:52:13.618925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shared pipeline function for all models\n",
    "def model_pipeline(train, dev, eval, excluded_columns, model, model_name, eval_metr_params):\n",
    "    feature_columns = [col for col in train.columns if col not in excluded_columns]\n",
    "\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "\n",
    "    train = impute_missing_values(train, feature_columns)\n",
    "    dev = impute_missing_values(dev, feature_columns)\n",
    "    eval = impute_missing_values(eval, feature_columns)\n",
    "\n",
    "    X_train, y_train = train[feature_columns], train['label']\n",
    "    X_dev, y_dev = dev[feature_columns], dev['label']\n",
    "    X_eval, y_eval = eval[feature_columns], eval['label']\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    y_eval_pred = model.predict(X_eval)\n",
    "    y_eval_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, *eval_metr_params['eval'])\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    # Feature Importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"\\n=== {model_name} Feature Importance ===\")\n",
    "        print(feature_importance.head(20))\n",
    "        # Plot top 20 features\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 20 Features - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"Feature importance plot saved as {model_name.lower()}_feature_importance_plot.png\")\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance extraction.\")\n",
    "\n",
    "# Example usage:\n",
    "eval_metr_params = {\n",
    "    'dev': [0.1588, 2.1007],\n",
    "    'eval': [0.1847, 2.0173]\n",
    "}\n",
    "\n",
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T04:52:14.864869Z",
     "iopub.status.busy": "2025-01-12T04:52:14.864517Z",
     "iopub.status.idle": "2025-01-12T04:52:34.831786Z",
     "shell.execute_reply": "2025-01-12T04:52:34.830735Z",
     "shell.execute_reply.started": "2025-01-12T04:52:14.864837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, random_forest, \"Random Forest\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T04:52:34.833404Z",
     "iopub.status.busy": "2025-01-12T04:52:34.833075Z",
     "iopub.status.idle": "2025-01-12T04:52:47.358759Z",
     "shell.execute_reply": "2025-01-12T04:52:47.357837Z",
     "shell.execute_reply.started": "2025-01-12T04:52:34.833375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, adaboost, \"AdaBoost\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T04:52:47.360704Z",
     "iopub.status.busy": "2025-01-12T04:52:47.360406Z",
     "iopub.status.idle": "2025-01-12T04:52:49.390766Z",
     "shell.execute_reply": "2025-01-12T04:52:49.389773Z",
     "shell.execute_reply.started": "2025-01-12T04:52:47.360676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, xgboost, \"XGBoost\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Conclusion\n",
    "\n",
    "|Feature|XGBoost Importance|Random Forest Importance|AdaBoost Importance|Selected?|\n",
    "|-|-|-|-|-|\n",
    "|minfun\t|0.119968\t|0.100231\t|0.22\t||\n",
    "|spectral_entropy\t|0.119771\t|0.106699\t|0.04\t||\n",
    "|meanfun\t|0.049779\t|0.101235\t|0.08\t||\n",
    "|mode_frequency\t|0.092469\t|0.089528\t|0.08\t||\n",
    "|bit_rate\t|0.027392\t|0.075031\t|0.06\t||\n",
    "|peak_frequency\t|N/A\t|0.075427\t|0.20\t||\n",
    "|energy\t|0.032100\t|0.075796\t|N/A\t||\n",
    "|zcr\t|0.029568\t|0.046573\t|0.04\t||\n",
    "|modindex\t|0.020655\t|0.035107\t|0.04\t||\n",
    "|dfrange\t|0.020519\t|0.021332\t|0.06\t||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model with Top 10 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T05:08:53.159013Z",
     "iopub.status.busy": "2025-01-12T05:08:53.158622Z",
     "iopub.status.idle": "2025-01-12T05:08:53.172290Z",
     "shell.execute_reply": "2025-01-12T05:08:53.171305Z",
     "shell.execute_reply.started": "2025-01-12T05:08:53.158979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shared pipeline function for all models\n",
    "def model_pipeline(train, dev, eval, excluded_columns, model, model_name, eval_metr_params):\n",
    "    selected_features = [\n",
    "        \"minfun\", \"spectral_entropy\", \"meanfun\", \"mode_frequency\", \"bit_rate\", \n",
    "        \"peak_frequency\", \"energy\", \"zcr\", \"modindex\", \"dfrange\"\n",
    "    ]\n",
    "\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "\n",
    "    train = impute_missing_values(train, selected_features)\n",
    "    dev = impute_missing_values(dev, selected_features)\n",
    "    eval = impute_missing_values(eval, selected_features)\n",
    "\n",
    "    X_train, y_train = train[selected_features], train['label']\n",
    "    X_dev, y_dev = dev[selected_features], dev['label']\n",
    "    X_eval, y_eval = eval[selected_features], eval['label']\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    y_dev_pred = model.predict(X_dev)\n",
    "    y_dev_prob = model.predict_proba(X_dev)[:, 1]\n",
    "    accuracy_dev = accuracy_score(y_dev, y_dev_pred)\n",
    "    eer_dev, tdcf_dev = eval_metr(y_dev, y_dev_prob, *eval_metr_params['dev'])\n",
    "    dev_report = classification_report(y_dev, y_dev_pred)\n",
    "\n",
    "    y_eval_pred = model.predict(X_eval)\n",
    "    y_eval_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, *eval_metr_params['eval'])\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation on Dev Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_dev:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(dev_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on validation data: {eer_dev * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on validation data: {tdcf_dev:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation on Eval Set ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "    joblib.dump(model, f'{model_name.lower()}_model.pkl')\n",
    "    print(f\"{model_name} model saved as {model_name.lower()}_model.pkl\")\n",
    "\n",
    "    # Feature Importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': selected_features,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(f\"\\n=== {model_name} Feature Importance ===\")\n",
    "        print(feature_importance)\n",
    "        feature_importance.to_csv(f'{model_name.lower()}_feature_importance.csv', index=False)\n",
    "        print(f\"Feature importance saved as {model_name.lower()}_feature_importance.csv\")\n",
    "\n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        bars = plt.barh(feature_importance['Feature'][::-1], feature_importance['Importance'][::-1], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top Features - {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{model_name.lower()}_feature_importance_plot.png')\n",
    "        plt.show()\n",
    "        print(f\"Feature importance plot saved as {model_name.lower()}_feature_importance_plot.png\")\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance extraction.\")\n",
    "\n",
    "# Example usage:\n",
    "eval_metr_params = {\n",
    "    'dev': [0.1588, 2.1007],\n",
    "    'eval': [0.1847, 2.0173]\n",
    "}\n",
    "\n",
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T05:08:59.876720Z",
     "iopub.status.busy": "2025-01-12T05:08:59.876392Z",
     "iopub.status.idle": "2025-01-12T05:09:14.341837Z",
     "shell.execute_reply": "2025-01-12T05:09:14.340855Z",
     "shell.execute_reply.started": "2025-01-12T05:08:59.876694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, random_forest, \"Random Forest\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T05:09:14.343683Z",
     "iopub.status.busy": "2025-01-12T05:09:14.343204Z",
     "iopub.status.idle": "2025-01-12T05:09:18.879064Z",
     "shell.execute_reply": "2025-01-12T05:09:18.878030Z",
     "shell.execute_reply.started": "2025-01-12T05:09:14.343596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, adaboost, \"AdaBoost\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T05:09:18.880990Z",
     "iopub.status.busy": "2025-01-12T05:09:18.880619Z",
     "iopub.status.idle": "2025-01-12T05:09:19.947974Z",
     "shell.execute_reply": "2025-01-12T05:09:19.946925Z",
     "shell.execute_reply.started": "2025-01-12T05:09:18.880952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model_pipeline(train, dev, eval, excluded_columns, xgboost, \"XGBoost\", eval_metr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:14:08.707527Z",
     "iopub.status.busy": "2025-01-12T07:14:08.707187Z",
     "iopub.status.idle": "2025-01-12T07:14:08.718749Z",
     "shell.execute_reply": "2025-01-12T07:14:08.717645Z",
     "shell.execute_reply.started": "2025-01-12T07:14:08.707501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shared pipeline function for all models with oversampling and undersampling options\n",
    "def model_pipeline_balanced(train, dev, eval, excluded_columns, model, model_name, eval_metr_params, resampling_method='smote'):\n",
    "    selected_features = [\n",
    "        \"minfun\", \"spectral_entropy\", \"meanfun\", \"mode_frequency\", \"bit_rate\", \n",
    "        \"peak_frequency\", \"energy\", \"zcr\", \"modindex\", \"dfrange\"\n",
    "    ]\n",
    "\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        for col in feature_columns:\n",
    "            if df[col].isna().any():\n",
    "                print(f\"Column '{col}' contains NaN values. Filling with mean.\")\n",
    "                df[col].fillna(df[col].mean(), inplace=True)\n",
    "        return df\n",
    "\n",
    "    train = impute_missing_values(train, selected_features)\n",
    "    dev = impute_missing_values(dev, selected_features)\n",
    "    eval = impute_missing_values(eval, selected_features)\n",
    "\n",
    "    X_train, y_train = train[selected_features], train['label']\n",
    "    X_dev, y_dev = dev[selected_features], dev['label']\n",
    "    X_eval, y_eval = eval[selected_features], eval['label']\n",
    "\n",
    "    # Apply resampling method (SMOTE or undersampling)\n",
    "    if resampling_method == 'smote':\n",
    "        resampler = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'undersample':\n",
    "        resampler = RandomUnderSampler(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = resampler.fit_resample(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resampling method: {resampling_method}\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Evaluate on the evaluation set\n",
    "    y_eval_pred = model.predict(X_eval)\n",
    "    y_eval_prob = model.predict_proba(X_eval)[:, 1]\n",
    "    accuracy_eval = accuracy_score(y_eval, y_eval_pred)\n",
    "    eer_eval, tdcf_eval = eval_metr(y_eval, y_eval_prob, *eval_metr_params['eval'])\n",
    "    eval_report = classification_report(y_eval, y_eval_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation on Eval Set using {resampling_method.upper()} ===\")\n",
    "    print(f\"Accuracy: {accuracy_eval:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(eval_report)\n",
    "    print(\"Custom Eval Metrics:\")\n",
    "    print(f\"EER on testing data: {eer_eval * 100:.2f}%\")\n",
    "    print(f\"Min t-DCF on testing data: {tdcf_eval:.4f}\")\n",
    "\n",
    "# Example usage with both SMOTE and undersampling:\n",
    "eval_metr_params = {\n",
    "    'dev': [0.1588, 2.1007],\n",
    "    'eval': [0.1847, 2.0173]\n",
    "}\n",
    "\n",
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:16:51.662543Z",
     "iopub.status.busy": "2025-01-12T07:16:51.662201Z",
     "iopub.status.idle": "2025-01-12T07:16:54.617434Z",
     "shell.execute_reply": "2025-01-12T07:16:54.616090Z",
     "shell.execute_reply.started": "2025-01-12T07:16:51.662516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "model_pipeline_balanced(train, dev, eval, excluded_columns, rf_model, \"Random Forest\", eval_metr_params, resampling_method='undersample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:16:54.619408Z",
     "iopub.status.busy": "2025-01-12T07:16:54.619034Z",
     "iopub.status.idle": "2025-01-12T07:16:55.865008Z",
     "shell.execute_reply": "2025-01-12T07:16:55.863923Z",
     "shell.execute_reply.started": "2025-01-12T07:16:54.619373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "model_pipeline_balanced(train, dev, eval, excluded_columns, adaboost, \"AdaBoost\", eval_metr_params, resampling_method='undersample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T07:16:55.867116Z",
     "iopub.status.busy": "2025-01-12T07:16:55.866696Z",
     "iopub.status.idle": "2025-01-12T07:16:57.102354Z",
     "shell.execute_reply": "2025-01-12T07:16:57.101499Z",
     "shell.execute_reply.started": "2025-01-12T07:16:55.867061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model_pipeline_balanced(train, dev, eval, excluded_columns, adaboost, \"XGBoost\", eval_metr_params, resampling_method='undersample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. System Specific Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 A01-A06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:39:16.429545Z",
     "iopub.status.busy": "2025-03-10T12:39:16.429037Z",
     "iopub.status.idle": "2025-03-10T12:39:17.151972Z",
     "shell.execute_reply": "2025-03-10T12:39:17.150692Z",
     "shell.execute_reply.started": "2025-03-10T12:39:16.429513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ganti dataset ngikut upload nnti\n",
    "train = pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "dev = pd.read_csv(\"/kaggle/input/borzi-full/dev_set.csv\")\n",
    "\n",
    "# Ganti value label\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "dev['label'] = dev['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "\n",
    "data = pd.concat([train, dev], axis=0, ignore_index=True, join='outer')\n",
    "\n",
    "# Drop col gk penting\n",
    "data = data.drop(['AUDIO_FILE_NAME', \"duration\", \"size\", \"spectral_bandwidth\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:39:19.107648Z",
     "iopub.status.busy": "2025-03-10T12:39:19.107252Z",
     "iopub.status.idle": "2025-03-10T12:39:19.156420Z",
     "shell.execute_reply": "2025-03-10T12:39:19.155391Z",
     "shell.execute_reply.started": "2025-03-10T12:39:19.107614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:39:20.275765Z",
     "iopub.status.busy": "2025-03-10T12:39:20.275426Z",
     "iopub.status.idle": "2025-03-10T12:39:20.281735Z",
     "shell.execute_reply": "2025-03-10T12:39:20.280800Z",
     "shell.execute_reply.started": "2025-03-10T12:39:20.275735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Evaluation & Feature Importance\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:33:38.167582Z",
     "iopub.status.busy": "2025-01-27T07:33:38.167111Z",
     "iopub.status.idle": "2025-01-27T07:33:38.186329Z",
     "shell.execute_reply": "2025-01-27T07:33:38.184922Z",
     "shell.execute_reply.started": "2025-01-27T07:33:38.167526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(f\"EER: {eer * 100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train and test models on all SYSTEM_IDs\n",
    "def train_and_test_all_system_ids(data):\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "    \n",
    "    # Get unique SYSTEM_IDs\n",
    "    system_ids = data['SYSTEM_ID'].unique()\n",
    "    system_ids = [sid for sid in system_ids if sid != 'bonafide']  # Exclude 'bonafide'\n",
    "\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store trained models for feature importance\n",
    "    trained_models = {}\n",
    "\n",
    "    for sid in system_ids:\n",
    "        print(f\"\\n=== SYSTEM_ID: {sid} ===\")\n",
    "        \n",
    "        # Filter data for bonafide and the current SYSTEM_ID\n",
    "        subset = data[(data['SYSTEM_ID'] == sid) | (data['SYSTEM_ID'] == 'bonafide')]\n",
    "        \n",
    "        # Show distribution of labels\n",
    "        # print(subset['SYSTEM_ID'].value_counts())\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X = subset.drop(['label', 'SYSTEM_ID'], axis=1)\n",
    "        y = subset['label']\n",
    "        feature_columns = X.columns  # Dynamically extract features for this subset\n",
    "\n",
    "        # Print feature names and preview the data\n",
    "        # print(f\"Features for SYSTEM_ID {sid}: {list(feature_columns)}\")\n",
    "        # print(f\"First rows of features for SYSTEM_ID {sid}:\\n{X.head()}\")\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n--- Evaluating {model_name} for SYSTEM_ID: {sid} ---\")\n",
    "            trained_model = evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "            print('\\n')\n",
    "            # Print trained model parameters\n",
    "            # print(f\"Trained {model_name} parameters for SYSTEM_ID {sid}:\\n{trained_model.get_params()}\")\n",
    "            \n",
    "            # Extract and print feature importances\n",
    "            if hasattr(trained_model, 'feature_importances_'):\n",
    "                feature_importances = trained_model.feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_columns,\n",
    "                    'Importance': feature_importances\n",
    "                }).sort_values(by='Importance', ascending=False).head(20)\n",
    "                print(f\"Top 20 Feature Importances for {model_name}, SYSTEM_ID {sid}:\\n{feature_importance_df}\")\n",
    "            \n",
    "            # Store the trained model and subset-specific features\n",
    "            trained_models[(sid, model_name)] = (trained_model, feature_columns)\n",
    "    \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:33:39.467619Z",
     "iopub.status.busy": "2025-01-27T07:33:39.467191Z",
     "iopub.status.idle": "2025-01-27T07:34:18.320023Z",
     "shell.execute_reply": "2025-01-27T07:34:18.319015Z",
     "shell.execute_reply.started": "2025-01-27T07:33:39.467585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_models = train_and_test_all_system_ids(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 A07-A13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:29:57.894952Z",
     "iopub.status.busy": "2025-01-27T07:29:57.894622Z",
     "iopub.status.idle": "2025-01-27T07:30:00.531330Z",
     "shell.execute_reply": "2025-01-27T07:30:00.530292Z",
     "shell.execute_reply.started": "2025-01-27T07:29:57.894918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ganti dataset ngikut upload nnti\n",
    "train= pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "eval = pd.read_csv(\"/kaggle/input/borzi-full/eval_set.csv\")\n",
    "\n",
    "# Ganti value label\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "eval['label'] = eval['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "\n",
    "data = pd.concat([train, eval], axis=0, ignore_index=True, join='outer')\n",
    "\n",
    "# Drop col gk penting\n",
    "data = data.drop(['AUDIO_FILE_NAME', \"duration\", \"size\", \"spectral_bandwidth\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:30:00.532576Z",
     "iopub.status.busy": "2025-01-27T07:30:00.532284Z",
     "iopub.status.idle": "2025-01-27T07:30:01.227820Z",
     "shell.execute_reply": "2025-01-27T07:30:01.226698Z",
     "shell.execute_reply.started": "2025-01-27T07:30:00.532530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T06:29:41.265084Z",
     "iopub.status.busy": "2025-01-27T06:29:41.264539Z",
     "iopub.status.idle": "2025-01-27T06:29:41.273503Z",
     "shell.execute_reply": "2025-01-27T06:29:41.271935Z",
     "shell.execute_reply.started": "2025-01-27T06:29:41.265048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:30:01.229413Z",
     "iopub.status.busy": "2025-01-27T07:30:01.228896Z",
     "iopub.status.idle": "2025-01-27T07:30:02.069878Z",
     "shell.execute_reply": "2025-01-27T07:30:02.068830Z",
     "shell.execute_reply.started": "2025-01-27T07:30:01.229382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(f\"EER: {eer * 100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "def feature_importance_analysis(model, model_name, feature_columns, system_id):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n=== Top 20 Features for SYSTEM_ID: {system_id} ({model_name}) ===\")\n",
    "        print(feature_importance.head(20))\n",
    "        \n",
    "        # Plot top 20 features\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 20 Features - {system_id} ({model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance extraction.\")\n",
    "\n",
    "\n",
    "# Train and test models on all SYSTEM_IDs\n",
    "def train_and_test_all_system_ids(data):\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "    \n",
    "    # Get unique SYSTEM_IDs\n",
    "    system_ids = data['SYSTEM_ID'].unique()\n",
    "    system_ids = [sid for sid in system_ids if sid != 'bonafide']  # Exclude 'bonafide'\n",
    "\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store trained models for feature importance\n",
    "    trained_models = {}\n",
    "\n",
    "    for sid in system_ids:\n",
    "        print(f\"\\n=== SYSTEM_ID: {sid} ===\")\n",
    "        \n",
    "        # Filter data for bonafide and the current SYSTEM_ID\n",
    "        subset = data[(data['SYSTEM_ID'] == sid) | (data['SYSTEM_ID'] == 'bonafide')]\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X = subset.drop(['label', 'SYSTEM_ID'], axis=1)\n",
    "        y = subset['label']\n",
    "        feature_columns = X.columns\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n--- Evaluating {model_name} for SYSTEM_ID: {sid} ---\")\n",
    "            trained_model = evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "            \n",
    "            # Store the trained model and feature columns\n",
    "            trained_models[(sid, model_name)] = (trained_model, feature_columns)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "\n",
    "# Run feature importance analysis for all trained models\n",
    "def analyze_feature_importance(trained_models):\n",
    "    for (sid, model_name), (model, feature_columns) in trained_models.items():\n",
    "        feature_importance_analysis(model, model_name, feature_columns, sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T06:29:44.526117Z",
     "iopub.status.busy": "2025-01-27T06:29:44.525732Z",
     "iopub.status.idle": "2025-01-27T06:32:13.901269Z",
     "shell.execute_reply": "2025-01-27T06:32:13.900151Z",
     "shell.execute_reply.started": "2025-01-27T06:29:44.526084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_models = train_and_test_all_system_ids(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T06:32:13.903074Z",
     "iopub.status.busy": "2025-01-27T06:32:13.902706Z",
     "iopub.status.idle": "2025-01-27T06:32:40.320650Z",
     "shell.execute_reply": "2025-01-27T06:32:40.319414Z",
     "shell.execute_reply.started": "2025-01-27T06:32:13.903044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "analyze_feature_importance(trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:30:02.072265Z",
     "iopub.status.busy": "2025-01-27T07:30:02.071881Z",
     "iopub.status.idle": "2025-01-27T07:30:02.087208Z",
     "shell.execute_reply": "2025-01-27T07:30:02.086001Z",
     "shell.execute_reply.started": "2025-01-27T07:30:02.072235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print(f\"EER: {eer * 100:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Train and test models on all SYSTEM_IDs\n",
    "def train_and_test_all_system_ids(data):\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "    \n",
    "    # Get unique SYSTEM_IDs\n",
    "    system_ids = data['SYSTEM_ID'].unique()\n",
    "    system_ids = [sid for sid in system_ids if sid != 'bonafide']  # Exclude 'bonafide'\n",
    "\n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    # Dictionary to store trained models for feature importance\n",
    "    trained_models = {}\n",
    "\n",
    "    for sid in system_ids:\n",
    "        print(f\"\\n=== SYSTEM_ID: {sid} ===\")\n",
    "        \n",
    "        # Filter data for bonafide and the current SYSTEM_ID\n",
    "        subset = data[(data['SYSTEM_ID'] == sid) | (data['SYSTEM_ID'] == 'bonafide')]\n",
    "        \n",
    "        # Show distribution of labels\n",
    "        # print(subset['SYSTEM_ID'].value_counts())\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X = subset.drop(['label', 'SYSTEM_ID'], axis=1)\n",
    "        y = subset['label']\n",
    "        feature_columns = X.columns  # Dynamically extract features for this subset\n",
    "\n",
    "        # Print feature names and preview the data\n",
    "        # print(f\"Features for SYSTEM_ID {sid}: {list(feature_columns)}\")\n",
    "        # print(f\"First rows of features for SYSTEM_ID {sid}:\\n{X.head()}\")\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n--- Evaluating {model_name} for SYSTEM_ID: {sid} ---\")\n",
    "            trained_model = evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "            print('\\n')\n",
    "            # Print trained model parameters\n",
    "            # print(f\"Trained {model_name} parameters for SYSTEM_ID {sid}:\\n{trained_model.get_params()}\")\n",
    "            \n",
    "            # Extract and print feature importances\n",
    "            if hasattr(trained_model, 'feature_importances_'):\n",
    "                feature_importances = trained_model.feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_columns,\n",
    "                    'Importance': feature_importances\n",
    "                }).sort_values(by='Importance', ascending=False).head(20)\n",
    "                print(f\"Top 20 Feature Importances for {model_name}, SYSTEM_ID {sid}:\\n{feature_importance_df}\")\n",
    "            \n",
    "            # Store the trained model and subset-specific features\n",
    "            trained_models[(sid, model_name)] = (trained_model, feature_columns)\n",
    "    \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T07:30:29.973561Z",
     "iopub.status.busy": "2025-01-27T07:30:29.972991Z",
     "iopub.status.idle": "2025-01-27T07:33:02.040307Z",
     "shell.execute_reply": "2025-01-27T07:33:02.039317Z",
     "shell.execute_reply.started": "2025-01-27T07:30:29.973495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_models = train_and_test_all_system_ids(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Sorting Predictable SYSTEM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T05:14:34.665834Z",
     "iopub.status.busy": "2025-02-01T05:14:34.665414Z",
     "iopub.status.idle": "2025-02-01T05:14:36.243542Z",
     "shell.execute_reply": "2025-02-01T05:14:36.242342Z",
     "shell.execute_reply.started": "2025-02-01T05:14:34.665799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ganti dataset ngikut upload nnti\n",
    "train= pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "eval = pd.read_csv(\"/kaggle/input/borzi-full/eval_set.csv\")\n",
    "\n",
    "# Ganti value label\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "eval['label'] = eval['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "\n",
    "data = pd.concat([train, eval], axis=0, ignore_index=True, join='outer')\n",
    "\n",
    "# Drop col gk penting\n",
    "data = data.drop(['AUDIO_FILE_NAME', \"duration\", \"size\", \"spectral_bandwidth\"], axis=1)\n",
    "\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T05:14:53.833293Z",
     "iopub.status.busy": "2025-02-01T05:14:53.832898Z",
     "iopub.status.idle": "2025-02-01T05:14:53.850138Z",
     "shell.execute_reply": "2025-02-01T05:14:53.848929Z",
     "shell.execute_reply.started": "2025-02-01T05:14:53.833256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Compute EER only if the model supports probability predictions\n",
    "    if y_pred_probs is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "        print(f\"EER: {eer * 100:.2f}%\")\n",
    "    else:\n",
    "        eer = None\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "def feature_importance_analysis(model, model_name, feature_columns, system_id):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n=== Top 20 Features for SYSTEM_ID: {system_id} ({model_name}) ===\")\n",
    "        print(feature_importance.head(20))\n",
    "        \n",
    "        # Plot top 20 features\n",
    "        top_features = feature_importance.head(20)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(top_features['Feature'][::-1], top_features['Importance'][::-1], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 20 Features - {system_id} ({model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance extraction.\")\n",
    "\n",
    "\n",
    "# Train and test models on all SYSTEM_IDs\n",
    "def train_and_test_all_system_ids(data):\n",
    "    # Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "    \n",
    "    # Get unique SYSTEM_IDs\n",
    "    system_ids = data['SYSTEM_ID'].unique()\n",
    "    system_ids = [sid for sid in system_ids if sid != 'bonafide']  # Exclude 'bonafide'\n",
    "\n",
    "    models = {\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    }\n",
    "\n",
    "    # Dictionary to store trained models for feature importance\n",
    "    trained_models = {}\n",
    "\n",
    "    for sid in system_ids:\n",
    "        print(f\"\\n=== SYSTEM_ID: {sid} ===\")\n",
    "        \n",
    "        # Filter data for bonafide and the current SYSTEM_ID\n",
    "        subset = data[(data['SYSTEM_ID'] == sid) | (data['SYSTEM_ID'] == 'bonafide')]\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X = subset.drop(['label', 'SYSTEM_ID'], axis=1)\n",
    "        y = subset['label']\n",
    "        feature_columns = X.columns\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n--- Evaluating {model_name} for SYSTEM_ID: {sid} ---\")\n",
    "            trained_model = evaluate_model(model, model_name, X_train, y_train, X_test, y_test)\n",
    "            \n",
    "            # Store the trained model and feature columns\n",
    "            trained_models[(sid, model_name)] = (trained_model, feature_columns)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "\n",
    "# Run feature importance analysis for all trained models\n",
    "def analyze_feature_importance(trained_models):\n",
    "    for (sid, model_name), (model, feature_columns) in trained_models.items():\n",
    "        feature_importance_analysis(model, model_name, feature_columns, sid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-01T03:14:52.534445Z",
     "iopub.status.busy": "2025-02-01T03:14:52.534119Z",
     "iopub.status.idle": "2025-02-01T03:17:49.215934Z",
     "shell.execute_reply": "2025-02-01T03:17:49.214795Z",
     "shell.execute_reply.started": "2025-02-01T03:14:52.534418Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_models = train_and_test_all_system_ids(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-01T05:43:15.977851Z",
     "iopub.status.busy": "2025-02-01T05:43:15.977512Z",
     "iopub.status.idle": "2025-02-01T05:43:15.993168Z",
     "shell.execute_reply": "2025-02-01T05:43:15.991813Z",
     "shell.execute_reply.started": "2025-02-01T05:43:15.977825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for saving results\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Store results in a list for Excel export\n",
    "evaluation_results = []\n",
    "\n",
    "# Modify evaluate_model to store results\n",
    "def evaluate_model(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n=== Evaluating {model_name} ===\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report_dict = classification_report(y_test, y_pred, output_dict=True)  # Convert to dictionary\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))  # Print for reference\n",
    "    print(f\"EER: {eer * 100:.2f}%\")\n",
    "    \n",
    "    # Convert classification report into a structured DataFrame\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.insert(0, \"Model\", model_name)\n",
    "    \n",
    "    # Add EER as a row in the report\n",
    "    eer_row = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"precision\": [None],\n",
    "        \"recall\": [None],\n",
    "        \"f1-score\": [None],\n",
    "        \"support\": [None],\n",
    "        \"EER\": [eer * 100]  # Convert EER to percentage\n",
    "    })\n",
    "    \n",
    "    # Append results (including EER)\n",
    "    evaluation_results.append(pd.concat([report_df, eer_row], ignore_index=True))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Modify feature_importance_analysis to save results\n",
    "feature_importance_data = []  # Store feature importance for export\n",
    "\n",
    "def feature_importance_analysis(model, model_name, feature_columns, system_id):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_columns,\n",
    "            'Importance': model.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        \n",
    "        # Store top 20 feature importances\n",
    "        for i, row in feature_importance.head(20).iterrows():\n",
    "            feature_importance_data.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Feature\": row[\"Feature\"],\n",
    "                \"Importance\": row[\"Importance\"]\n",
    "            })\n",
    "        \n",
    "        # Save feature importance plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(feature_importance[\"Feature\"][:20][::-1], feature_importance[\"Importance\"][:20][::-1], color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 20 Features - {system_id} ({model_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, f\"feature_importance_{system_id}_{model_name}.png\"))  # Save as image\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Feature importance plot saved: feature_importance_{system_id}_{model_name}.png\")\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importance extraction.\")\n",
    "\n",
    "\n",
    "# After training all models, save results to Excel\n",
    "def save_results_to_excel():\n",
    "    # Convert lists to DataFrames\n",
    "    if evaluation_results:\n",
    "        eval_df = pd.concat(evaluation_results, ignore_index=True)\n",
    "    else:\n",
    "        eval_df = pd.DataFrame()\n",
    "\n",
    "    if feature_importance_data:\n",
    "        feat_imp_df = pd.concat(feature_importance_data, ignore_index=True)\n",
    "    else:\n",
    "        feat_imp_df = pd.DataFrame()\n",
    "\n",
    "    # Debugging output\n",
    "    print(\"\\n===== Debug: Evaluation Results Sample =====\")\n",
    "    print(eval_df.head())\n",
    "\n",
    "    print(\"\\n===== Debug: Feature Importance Sample =====\")\n",
    "    print(feat_imp_df.head())\n",
    "\n",
    "    # Ensure there's data to save\n",
    "    if eval_df.empty and feat_imp_df.empty:\n",
    "        print(\" No data to save. Check if models ran correctly.\")\n",
    "        return\n",
    "\n",
    "    # Save to Excel\n",
    "    results_path = os.path.join(RESULTS_DIR, \"model_results.xlsx\")\n",
    "    with pd.ExcelWriter(results_path, engine='xlsxwriter') as writer:\n",
    "        if not eval_df.empty:\n",
    "            eval_df.to_excel(writer, sheet_name=\"Evaluation Results\", index=False)\n",
    "        if not feat_imp_df.empty:\n",
    "            feat_imp_df.to_excel(writer, sheet_name=\"Feature Importance\", index=False)\n",
    "\n",
    "    print(f\" Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-02-01T05:43:16.910995Z",
     "iopub.status.busy": "2025-02-01T05:43:16.910642Z",
     "iopub.status.idle": "2025-02-01T05:46:58.252548Z",
     "shell.execute_reply": "2025-02-01T05:46:58.251117Z",
     "shell.execute_reply.started": "2025-02-01T05:43:16.910965Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_models = train_and_test_all_system_ids(data)\n",
    "analyze_feature_importance(trained_models)\n",
    "save_results_to_excel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Select SYSTEM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T12:50:51.126346Z",
     "iopub.status.busy": "2025-03-10T12:50:51.125945Z",
     "iopub.status.idle": "2025-03-10T12:50:51.131536Z",
     "shell.execute_reply": "2025-03-10T12:50:51.130304Z",
     "shell.execute_reply.started": "2025-03-10T12:50:51.126314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:03:07.259540Z",
     "iopub.status.busy": "2025-03-10T13:03:07.259195Z",
     "iopub.status.idle": "2025-03-10T13:03:08.836741Z",
     "shell.execute_reply": "2025-03-10T13:03:08.835603Z",
     "shell.execute_reply.started": "2025-03-10T13:03:07.259514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "eval = pd.read_csv(\"/kaggle/input/borzi-full/eval_set.csv\")\n",
    "\n",
    "# Map labels\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "eval['label'] = eval['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "\n",
    "# Concatenate train and eval sets\n",
    "data = pd.concat([train, eval], axis=0, ignore_index=True, join='outer')\n",
    "\n",
    "# Handle missing values function\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "# Handle the missing value\n",
    "data = handle_missing_values(data)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(['AUDIO_FILE_NAME', \"duration\", \"size\", \"spectral_bandwidth\"], axis=1) # msrcc & psrcc karena null\n",
    "\n",
    "# Filter SYSTEM_ID to keep only specific values\n",
    "valid_system_ids = [\"A06\", \"A19\"]\n",
    "data = data[~data[\"SYSTEM_ID\"].isin(valid_system_ids)] # ~ buat exclude\n",
    "\n",
    "# Drop SYSTEM_ID column after filtering\n",
    "data = data.drop(columns=[\"SYSTEM_ID\"])\n",
    "\n",
    "# Split features and labels\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Normal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:03:08.838886Z",
     "iopub.status.busy": "2025-03-10T13:03:08.838413Z",
     "iopub.status.idle": "2025-03-10T13:04:14.839581Z",
     "shell.execute_reply": "2025-03-10T13:04:14.838435Z",
     "shell.execute_reply.started": "2025-03-10T13:03:08.838833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    eer, _ = eval_metr(y_test, y_pred, 0.1847, 2.0173)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{model_name} EER: {eer * 100:.2f}%\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:23:19.214229Z",
     "iopub.status.busy": "2025-02-02T13:23:19.213720Z",
     "iopub.status.idle": "2025-02-02T13:23:19.783263Z",
     "shell.execute_reply": "2025-02-02T13:23:19.781467Z",
     "shell.execute_reply.started": "2025-02-02T13:23:19.214195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)  # Adjust ratio as needed\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:23:31.941182Z",
     "iopub.status.busy": "2025-02-02T13:23:31.940618Z",
     "iopub.status.idle": "2025-02-02T13:24:22.920273Z",
     "shell.execute_reply": "2025-02-02T13:24:22.918652Z",
     "shell.execute_reply.started": "2025-02-02T13:23:31.941128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    eer, _ = eval_metr(y_test, y_pred, 0.1847, 2.0173)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{model_name} EER: {eer * 100:.2f}%\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:25:39.460914Z",
     "iopub.status.busy": "2025-02-02T13:25:39.460357Z",
     "iopub.status.idle": "2025-02-02T13:25:39.505551Z",
     "shell.execute_reply": "2025-02-02T13:25:39.503096Z",
     "shell.execute_reply.started": "2025-02-02T13:25:39.460874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersampler  = RandomUnderSampler(sampling_strategy=0.5, random_state=42)  # Adjust ratio as needed\n",
    "X_train_resampled, y_train_resampled = undersampler .fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:25:49.403671Z",
     "iopub.status.busy": "2025-02-02T13:25:49.403300Z",
     "iopub.status.idle": "2025-02-02T13:26:13.413095Z",
     "shell.execute_reply": "2025-02-02T13:26:13.412006Z",
     "shell.execute_reply.started": "2025-02-02T13:25:49.403643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    eer, _ = eval_metr(y_test, y_pred, 0.1847, 2.0173)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{model_name} EER: {eer * 100:.2f}%\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Not Joining Eval to the set (Eval is for testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:05:05.006554Z",
     "iopub.status.busy": "2025-03-10T13:05:05.006176Z",
     "iopub.status.idle": "2025-03-10T13:05:05.012279Z",
     "shell.execute_reply": "2025-03-10T13:05:05.010992Z",
     "shell.execute_reply.started": "2025-03-10T13:05:05.006520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:05:05.201082Z",
     "iopub.status.busy": "2025-03-10T13:05:05.200700Z",
     "iopub.status.idle": "2025-03-10T13:05:05.208344Z",
     "shell.execute_reply": "2025-03-10T13:05:05.207157Z",
     "shell.execute_reply.started": "2025-03-10T13:05:05.201050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_curve, accuracy_score\n",
    "def eval_metr(y_true, y_pred, C0, C1, P_target=0.5):\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    fnr = 1 - tpr  # False Negative Rate (Miss Rate)\n",
    "\n",
    "    # Compute EER\n",
    "    abs_diff = np.abs(fpr - fnr)\n",
    "    eer_index = np.argmin(abs_diff)\n",
    "    eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "\n",
    "    # Compute t-DCF\n",
    "    pi_spoof = P_target  # Prior for spoof\n",
    "    pi_bonafide = 1 - P_target  # Prior for bonafide\n",
    "\n",
    "    # Calculate t-DCF values\n",
    "    tdcf_values = (pi_bonafide * C0 * fnr + pi_spoof * C1 * fpr) / min(pi_bonafide * C0, pi_spoof * C1)\n",
    "\n",
    "    # Find minimum t-DCF\n",
    "    min_tdcf = np.min(tdcf_values)\n",
    "\n",
    "    return eer, min_tdcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:06:20.887051Z",
     "iopub.status.busy": "2025-03-10T13:06:20.886644Z",
     "iopub.status.idle": "2025-03-10T13:06:22.352662Z",
     "shell.execute_reply": "2025-03-10T13:06:22.351581Z",
     "shell.execute_reply.started": "2025-03-10T13:06:20.887018Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv(\"/kaggle/input/borzi-full/train_set.csv\")\n",
    "eval = pd.read_csv(\"/kaggle/input/borzi-full/eval_set.csv\")\n",
    "\n",
    "# Map labels\n",
    "train['label'] = train['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "eval['label'] = eval['label'].map({'bonafide': 1, 'spoof': 0})\n",
    "valid_system_ids = [\"A06\", \"A17\", \"A19\"]\n",
    "\n",
    "# Handle missing values function\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].isnull().sum() > 0:\n",
    "            if data[column].dtype in ['int64', 'float64']:\n",
    "                data[column].fillna(data[column].mean(), inplace=True)\n",
    "            else:\n",
    "                data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    return data\n",
    "\n",
    "def train_test_filter(df):\n",
    "    # Handle the missing value\n",
    "    df = handle_missing_values(df)\n",
    "    # Drop col null atau gk penting\n",
    "    df = df.drop(['AUDIO_FILE_NAME', \"duration\", \"size\", \"spectral_bandwidth\"], axis=1) # msrcc & psrcc karena null\n",
    "    # Filter System_id\n",
    "    df = df[~df[\"SYSTEM_ID\"].isin(valid_system_ids)] # ~ buat exclude\n",
    "    # Drop system_id setelah di filter\n",
    "    df = df.drop(columns=[\"SYSTEM_ID\"])\n",
    "\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = train_test_filter(train)\n",
    "X_test, y_test = train_test_filter(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:06:24.628706Z",
     "iopub.status.busy": "2025-03-10T13:06:24.628371Z",
     "iopub.status.idle": "2025-03-10T13:06:24.634777Z",
     "shell.execute_reply": "2025-03-10T13:06:24.633827Z",
     "shell.execute_reply.started": "2025-03-10T13:06:24.628679Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T13:06:26.837726Z",
     "iopub.status.busy": "2025-03-10T13:06:26.837371Z",
     "iopub.status.idle": "2025-03-10T13:06:50.727334Z",
     "shell.execute_reply": "2025-03-10T13:06:50.726467Z",
     "shell.execute_reply.started": "2025-03-10T13:06:26.837698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, n_estimators=100),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Loop through each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n===== {model_name} =====\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    eer, _ = eval_metr(y_test, y_pred, 0.1847, 2.0173)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{model_name} EER: {eer * 100:.2f}%\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Feature Importance on Spoofing Methods Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:15:03.024559Z",
     "iopub.status.busy": "2025-02-28T15:15:03.024204Z",
     "iopub.status.idle": "2025-02-28T15:15:03.035334Z",
     "shell.execute_reply": "2025-02-28T15:15:03.034096Z",
     "shell.execute_reply.started": "2025-02-28T15:15:03.024523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def process_and_select_features(train, dev, eval, excluded_columns, system_id_list):\n",
    "    # Combine datasets\n",
    "    data = pd.concat([train, dev, eval], ignore_index=True)\n",
    "    \n",
    "    # Filter based on system_id_list\n",
    "    data = data[data['SYSTEM_ID'].astype(str).isin(system_id_list)]\n",
    "    data = data.drop(columns=['SYSTEM_ID'])\n",
    "    \n",
    "    # Identify feature columns\n",
    "    feature_columns = [col for col in data.columns if col not in excluded_columns + ['label']]\n",
    "    \n",
    "    # Impute missing values\n",
    "    def impute_missing_values(df, feature_columns):\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        df[feature_columns] = imputer.fit_transform(df[feature_columns])\n",
    "        return df\n",
    "    \n",
    "    data = impute_missing_values(data, feature_columns)\n",
    "    \n",
    "    # Encode labels\n",
    "    data['label'] = data['label'].replace({\"bonafide\": 1, \"spoof\": 0})\n",
    "    data = data.dropna(subset=['label'])\n",
    "    data['label'] = data['label'].astype(int)\n",
    "    \n",
    "    # Feature importance calculation\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "    \n",
    "    X = data[feature_columns]\n",
    "    y = data['label']\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feature_importance_df['Num'] = range(1, 21)  # Top 20 features\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X, y)\n",
    "        feature_importance = model.feature_importances_\n",
    "        sorted_indices = np.argsort(feature_importance)[::-1][:20]  # Top 20 features\n",
    "        top_features = [feature_columns[i] for i in sorted_indices]\n",
    "        top_scores = [feature_importance[i] for i in sorted_indices]\n",
    "        \n",
    "        feature_importance_df[model_name] = [f\"{f} ({s:.4f})\" for f, s in zip(top_features, top_scores)]\n",
    "    \n",
    "    return feature_importance_df\n",
    "\n",
    "excluded_columns = [\"label\", \"duration\", \"size\", \"spectral_bandwidth\"]\n",
    "# system_id_list = [\"bonafide\", \"A01\", \"A02\", \"A03\", \"A04\", \"A07\", \"A08\", \"A09\", \"A10\", \"A11\", \"A12\", \"A16\"]  # TTS\n",
    "# system_id_list = [\"bonafide\", \"A05\", \"A06\", \"A17\", \"A18\", \"A19\"]  # VC\n",
    "system_id_list = [\"bonafide\", \"A13\", \"A14\", \"A15\"]  # TTS_VC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:15:03.349718Z",
     "iopub.status.busy": "2025-02-28T15:15:03.349333Z",
     "iopub.status.idle": "2025-02-28T15:15:24.734005Z",
     "shell.execute_reply": "2025-02-28T15:15:24.731554Z",
     "shell.execute_reply.started": "2025-02-28T15:15:03.349684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "process_and_select_features(train, dev, eval, excluded_columns, system_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2286778,
     "sourceId": 3842332,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6393030,
     "sourceId": 10386241,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6442128,
     "sourceId": 10469024,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
